{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class for Loading MNIST Data\n",
    "\n",
    "This code snippet defines a Python class, `Dataset`, responsible for loading the MNIST training and testing data from the filesystem. The MNIST data files are expected to be in the IDX file format.\n",
    "\n",
    "1. **Import Dependencies**: The required modules, `numpy as np` and `struct`, are imported at the beginning of the code to handle array operations and binary data reading, respectively.\n",
    "\n",
    "2. **Initialization**: The `__init__` method initializes the class and immediately calls methods to load the training and testing labels and images.\n",
    "\n",
    "3. **Reading Labels**: The `read_idx_labels` method reads the labels from an IDX formatted file. It reads the magic number and the number of items from the file header and then loads the labels into a NumPy array.\n",
    "\n",
    "4. **Reading Images**: Similar to `read_idx_labels`, the `read_idx_images` method reads image data from an IDX formatted file. It reads the magic number, the number of items, and the dimensions (rows and cols) of each image. The image data is loaded into a 3D NumPy array and normalized by dividing by 255.\n",
    "\n",
    "5. **Get Train and Test Data**: The `get_train_test_data` method returns the loaded training and testing image and label datasets.\n",
    "\n",
    "6. **Instantiation and Data Retrieval**: Finally, an instance of the `Dataset` class is created, and the training and testing data are retrieved using `get_train_test_data`.\n",
    "\n",
    "Note: While making `Dataset` a class might seem like overkill for this simple example, this approach stems from a larger project where the class had additional features and functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self) -> None:\n",
    "        self.train_labels = self.read_idx_labels(\"data/train-labels.idx1-ubyte\")\n",
    "        self.train_images = self.read_idx_images(\"data/train-images.idx3-ubyte\")\n",
    "        self.test_labels = self.read_idx_labels(\"data/t10k-labels.idx1-ubyte\")\n",
    "        self.test_images = self.read_idx_images(\"data/t10k-images.idx3-ubyte\")\n",
    "        \n",
    "    def read_idx_labels(self, file_path : str) -> np.ndarray:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            magic, num = struct.unpack(\">II\", f.read(8))\n",
    "            labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return labels\n",
    "\n",
    "    def read_idx_images(self, file_path : str) -> np.ndarray:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "            images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num, rows, cols)\n",
    "        return images.astype('float32')/255\n",
    "    \n",
    "    def get_train_test_data(self):\n",
    "        return self.train_images, self.train_labels, self.test_images, self.test_labels\n",
    "\n",
    "data = Dataset()\n",
    "X_train, y_train, X_test, y_test = data.get_train_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a PyTorch Neural Network\n",
    "\n",
    "This section demonstrates how to create a neural network model using PyTorch to solve the MNIST digit classification problem.\n",
    "\n",
    "1. **Import Dependencies**: We import the necessary PyTorch modules such as `torch`, `nn`, and `optim`.\n",
    "\n",
    "2. **Define the Model Class**: We create a PyTorch model class named `MyModel` that inherits from `nn.Module`. Inside the class, we:\n",
    "    - Define the layers in the `__init__` method. \n",
    "    - Implement the `forward` method to describe how data flows through the network.\n",
    "\n",
    "3. **Layers**:\n",
    "    - `Flatten`: To flatten the 28x28 input images.\n",
    "    - `fc1`: A fully connected layer with 128 units and ReLU activation.\n",
    "    - `fc2`: Another fully connected layer with 64 units and ReLU activation.\n",
    "    - `fc3`: The output fully connected layer with 10 units, corresponding to 10 classes. Softmax is applied in the `forward` method.\n",
    "\n",
    "4. **Model Initialization**: An instance of the `MyModel` class is created.\n",
    "\n",
    "5. **Optimizer and Loss Function**: \n",
    "    - `Adam` optimizer is used for optimizing the model parameters.\n",
    "    - `CrossEntropyLoss` is chosen as the loss function, suitable for multi-class classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "model = MyModel()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the PyTorch Model\n",
    "\n",
    "This section outlines the steps for training the PyTorch model on the MNIST dataset.\n",
    "\n",
    "1. **Convert Labels to Torch Tensors**: We convert the `y_train` and `y_test` labels to long PyTorch tensors.\n",
    "\n",
    "2. **Create Data Loaders**: \n",
    "    - The training dataset and dataloader are created using PyTorch's `TensorDataset` and `DataLoader` classes.\n",
    "    - Batch size is set to 128 and the dataset is shuffled before each epoch.\n",
    "\n",
    "3. **Training Loop**:\n",
    "    - We run the model for 10 epochs.\n",
    "    - The `zero_grad` method is called to reset gradients.\n",
    "    - Forward pass and loss computation are performed.\n",
    "    - Backward pass and optimization step are executed.\n",
    "\n",
    "4. **Time Monitoring**:\n",
    "    - We record the start and end time to calculate the total time taken for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 9.09 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import time\n",
    "\n",
    "y_train = torch.tensor(y_train).long()\n",
    "y_test = torch.tensor(y_test).long()\n",
    "\n",
    "train_dataset = data.TensorDataset(torch.tensor(X_train), y_train)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(f\"Time taken to fit the model: {time_taken:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation on Test Data\n",
    "\n",
    "This section focuses on evaluating the trained PyTorch model using the test dataset.\n",
    "\n",
    "1. **Test Data Loader**: Similar to the training data loader, a test data loader is created using PyTorch's `TensorDataset` and `DataLoader` classes. The batch size is set to 128.\n",
    "\n",
    "2. **Evaluation Loop**:\n",
    "    - We disable gradient calculation using `torch.no_grad()` to speed up the computation.\n",
    "    - Forward passes are performed on test batches to get the model's predictions.\n",
    "    - Predicted labels are compared with true labels to count the number of correct predictions.\n",
    "\n",
    "3. **Calculate Test Accuracy**:\n",
    "    - The accuracy on the test dataset is computed and printed in percentage format with two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.79%\n"
     ]
    }
   ],
   "source": [
    "test_dataset = data.TensorDataset(torch.tensor(X_test), y_test)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        output = model(X_batch)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print(f'Test accuracy: {test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
