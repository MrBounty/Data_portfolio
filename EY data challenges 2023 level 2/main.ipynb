{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eba2110-c3ad-4136-849e-14ffa70decd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from useful import *\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Others\n",
    "from tqdm.notebook import tqdm\n",
    "from ast import literal_eval\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62408168-094e-456b-a9b4-174f5c3bc62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of data sets to use for making predictions. \n",
    "# The code can take a combination of data sets 4, 5, 6 or 7\n",
    "data_name = [\"4\", \"5\"]\n",
    "random_state = 64\n",
    "\n",
    "# Column names to drop from the input data (can be improve)\n",
    "to_drop_full = [\"sentinel 1 date\", \"sentinel 2 date\", \"landsat date\", 'landsat qa_pixel', 'sentinel 2 SCL', \"sentinel 2 blue\", \"sentinel 2 green\", \"sentinel 2 red\",\n",
    "          \"sentinel 2 nir\", \"landsat blue\", \"landsat green\", \"landsat red\", \"landsat nir08\", \"landsat lwir11\",\n",
    "          'sentinel 2 ari1', 'sentinel 2 ari2', 'sentinel 2 cri1', 'sentinel 2 cri2', 'sentinel 2 rdvi',\n",
    "          'sentinel 2 gndvi', 'sentinel 2 savi', 'sentinel 2 evi', 'sentinel 2 tvi', 'sentinel 2 B05', 'sentinel 2 B06',\n",
    "       'sentinel 2 B07', 'sentinel 2 B8A', 'sentinel 1 rvi', 'sentinel 1 vh', 'sentinel 1 vv']\n",
    "\n",
    "to_drop_medium = [\"sentinel 1 date\", \"sentinel 2 date\", \"landsat date\", 'landsat qa_pixel', 'sentinel 2 SCL', \"sentinel 2 blue\", \"sentinel 2 green\", \"sentinel 2 red\"\n",
    "                  , \"landsat blue\", \"landsat green\", \"landsat red\", 'sentinel 2 ari1', 'sentinel 2 ari2', 'sentinel 2 cri1', 'sentinel 2 cri2', \"sentinel 2 nir\"]\n",
    "\n",
    "to_drop_small = [\"sentinel 1 date\", \"sentinel 2 date\", \"landsat date\", 'sentinel 2 SCL', 'landsat qa_pixel']\n",
    "\n",
    "# I don't remember which one is the best but to_drop_full and to_drop_medium should be similar and the best\n",
    "to_drop = to_drop_medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec58f4f1-31cb-4202-9c11-9e93b1f8ca84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5fa12d0f014cf2af02cc2eb6641c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/557 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba26f4a16d64ade9861333ac457ad73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/557 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crop_yield_data = pd.read_csv(\"Crop_Yield_Data_challenge_2.csv\")\n",
    "sat_datas = []\n",
    "\n",
    "# Import the previously imported and save dataset from the import data notebook\n",
    "for name in data_name:\n",
    "    sat_datas.append(pd.read_csv(\"data/data_\" + name + \".csv\").drop(to_drop, axis=1))\n",
    "\n",
    "# Remove nan value and transform string of list into a list (can be improve)\n",
    "for col in sat_datas[0].columns:\n",
    "    for sat_data in sat_datas:\n",
    "        sat_data[col] = sat_data[col].str.replace(', nan', '')\n",
    "        sat_data[col] = sat_data[col].str.replace('nan, ', '')\n",
    "        sat_data[col] = sat_data[col].apply(literal_eval)\n",
    "\n",
    "# Extract the statistical features of each dataset (can be improve)\n",
    "features = []\n",
    "for sat_data in sat_datas:\n",
    "    features.append(np.array(generate_statistical_features_v4(sat_data)))\n",
    "\n",
    "# Concatenate everything\n",
    "y = crop_yield_data['Rice Yield (kg/ha)'].values\n",
    "X = np.concatenate(features, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a0e8e9-4ab4-4a91-b981-006630409174",
   "metadata": {},
   "source": [
    "## Multiple regression type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e39b69e9-d367-45a4-8753-cd99a057d8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d828475a9f465ebf536ff97a2b1901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In this code block I will train 100 time for different random_state, 4 different model and get theirs score\n",
    "# I will then find the x best one and train them again using another random_state for the data split but the same random_state for the model\n",
    "# I will then use them to get the submission\n",
    "\n",
    "scores = []\n",
    "pbar = tqdm(range(100))\n",
    "for random_state in pbar:\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    # Scale the data\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \n",
    "    # Regressors to train, parameter found with RandomizedSearchCV (can be improve)\n",
    "    regressors = [\n",
    "        [\"XGBRegressor\", XGBRegressor(\n",
    "            random_state=random_state,\n",
    "            colsample_bytree=.7,\n",
    "            gamma=0,\n",
    "            learning_rate=.05,\n",
    "            max_depth=3,\n",
    "            min_child_weight=1,\n",
    "            n_estimators=100,\n",
    "            subsample=.8)],\n",
    "        [\"ElasticNet\", ElasticNet(\n",
    "            random_state=random_state,\n",
    "            alpha=0.01,\n",
    "            l1_ratio=0.8,\n",
    "            max_iter=1000,\n",
    "            tol=0.1)],\n",
    "        [\"GradientBoostingRegressor\", GradientBoostingRegressor(\n",
    "            random_state=random_state, \n",
    "            criterion='squared_error', \n",
    "            learning_rate=0.05, \n",
    "            max_depth=3, \n",
    "            n_estimators=150,\n",
    "            max_features=1,\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=3,\n",
    "            subsample=.6)],\n",
    "        [\"RandomForestRegressor\", RandomForestRegressor(\n",
    "            random_state=random_state,\n",
    "            n_estimators=50, \n",
    "            max_depth=3,\n",
    "            bootstrap=False,\n",
    "            max_features=\"log2\",\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=5)]\n",
    "    ]\n",
    "    \n",
    "    # train and evaluate each model\n",
    "    for regressor in regressors:\n",
    "        pbar.set_description(\"Processing {} for random_state {}\".format(regressor[0], random_state))\n",
    "        regressor[1].fit(X_train, y_train)\n",
    "        s_in, s_out = get_score(regressor[1], X_train, y_train, X_test, y_test, False)\n",
    "        scores.append([regressor[0], random_state, s_in, s_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abc93a11-d830-40f0-8f5e-7db9b9cb49b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean in: 0.8536755660382582, mean out: 0.6521361212612233\n"
     ]
    }
   ],
   "source": [
    "# Find the 150 best random state using mean score of train data and 2 * test data\n",
    "n_best = 150\n",
    "best_items = sorted(scores, key=lambda x: (x[3] + 2*x[2]), reverse=True)[:n_best]\n",
    "\n",
    "total_in = 0\n",
    "total_out = 0\n",
    "for nested_list in best_items:\n",
    "    total_in += nested_list[2]\n",
    "    total_out += nested_list[3]\n",
    "\n",
    "mean_in = total_in / len(best_items)\n",
    "mean_out = total_out / len(best_items)\n",
    "\n",
    "print(\"Mean in: {}, mean out: {}\".format(mean_in, mean_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50df113-839e-40f0-8196-d92eca00286a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9704779517724f968e190c911a85d9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the 150 best model using a new random_state for slipt\n",
    "regressors = []\n",
    "scs = []\n",
    "pbar = tqdm(best_items)\n",
    "for s in pbar:\n",
    "    random_state = s[1]\n",
    "    pbar.set_description(\"Processing {} for random_state {}\".format(s[0], random_state))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.0001, random_state=random_state+1)\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    scs.append(sc)\n",
    "    if s[0] == \"RandomForestRegressor\":\n",
    "        regressor = RandomForestRegressor(\n",
    "            random_state=random_state,\n",
    "            n_estimators=50, \n",
    "            max_depth=3,\n",
    "            bootstrap=False,\n",
    "            max_features=\"log2\",\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=5)\n",
    "    elif s[0] == \"XGBRegressor\":\n",
    "        regressor = XGBRegressor(\n",
    "            random_state=random_state,\n",
    "            colsample_bytree=.7,\n",
    "            gamma=0,\n",
    "            learning_rate=.05,\n",
    "            max_depth=3,\n",
    "            min_child_weight=1,\n",
    "            n_estimators=100,\n",
    "            subsample=.8)\n",
    "    elif s[0] == \"GradientBoostingRegressor\":\n",
    "        regressor = GradientBoostingRegressor(\n",
    "            random_state=random_state, \n",
    "            criterion='squared_error', \n",
    "            learning_rate=0.05, \n",
    "            max_depth=3, \n",
    "            n_estimators=150,\n",
    "            max_features=1,\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=3,\n",
    "            subsample=.6)\n",
    "    else:\n",
    "        regressor = ElasticNet(\n",
    "            random_state=random_state,\n",
    "            alpha=0.01,\n",
    "            l1_ration=0.8,\n",
    "            max_iter=1000,\n",
    "            tol=0.1)\n",
    "        \n",
    "    regressor.fit(X_train, y_train)\n",
    "    regressors.append(regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e57118-3bfe-4d6d-95bd-5d63c5e778ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models to a picle file\n",
    "import pickle\n",
    "pickle.dump(regressors, open('model.pkl', 'wb'))\n",
    "pickle.dump(scs, open('scalers.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cde6de-bb4e-4fe3-9fdf-1f9238564e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
